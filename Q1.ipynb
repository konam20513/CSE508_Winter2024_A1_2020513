{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing required pipelines\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading English CPU piepline from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def pre_pro(text):\n",
    "    \n",
    "    # lowercasing the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Convert it into spacy format\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Inclduing tokens which are not stopwords, punctuations and blanks\n",
    "    fil_tok = [tok.text for tok in doc if not tok.is_stop and not tok.is_punct and not tok.is_space]\n",
    "    \n",
    "    # joining them and converting back to string\n",
    "    return ' '.join(fil_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automating reading files, preproessing and saving them\n",
    "def process_files(inp, op):\n",
    "    \n",
    "    # creating output folder\n",
    "    os.makedirs(op, exist_ok=True)\n",
    "\n",
    "    for file_name in os.listdir(inp):\n",
    "        if file_name.endswith('.txt'):\n",
    "            \n",
    "            # getting path of input file\n",
    "            ip_path = os.path.join(inp, file_name)\n",
    "            \n",
    "            # getting path of output file\n",
    "            op_path = os.path.join(op, file_name)\n",
    "            \n",
    "            # reading the file with utf8 encoding\n",
    "            with open(ip_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "            # passing the text to above defined preprocessing function\n",
    "            processed = pre_pro(text)\n",
    "            \n",
    "            # writing the processed text into new file\n",
    "            with open(op_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output folder paths\n",
    "input_folder = \"text\"\n",
    "output_folder = \"preprocessed_text2\"\n",
    "\n",
    "process_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- File: file1.txt ---\n",
      "\n",
      "Original Text:\n",
      " Loving these vintage springs on my vintage strat. They have a good tension and great stability. If you are floating your bridge and want the most out of your springs than these are the way to go.\n",
      "\n",
      "After Lowercasing:\n",
      " loving these vintage springs on my vintage strat. they have a good tension and great stability. if you are floating your bridge and want the most out of your springs than these are the way to go.\n",
      "\n",
      "After Tokenization:\n",
      " loving these vintage springs on my vintage strat . they have a good tension and great stability . if you are floating your bridge and want the most out of your springs than these are the way to go .\n",
      "\n",
      "After Removing Stopwords:\n",
      " loving vintage springs vintage strat . good tension great stability . floating bridge want springs way .\n",
      "\n",
      "After Removing Punctuations:\n",
      " loving vintage springs vintage strat good tension great stability floating bridge want springs way\n",
      "\n",
      "After Removing Blank Spaces:\n",
      " loving vintage springs vintage strat good tension great stability floating bridge want springs way\n",
      "\n",
      "Final Processed Sentence:\n",
      " loving vintage springs vintage strat good tension great stability floating bridge want springs way\n",
      "\n",
      "--- File: file2.txt ---\n",
      "\n",
      "Original Text:\n",
      " Works great as a guitar bench mat. Not rugged enough for abuse but if you take care of it, it will take care of you. Makes organization of workspace much easier because screws won't roll around. Color is good too.\n",
      "\n",
      "After Lowercasing:\n",
      " works great as a guitar bench mat. not rugged enough for abuse but if you take care of it, it will take care of you. makes organization of workspace much easier because screws won't roll around. color is good too.\n",
      "\n",
      "After Tokenization:\n",
      " works great as a guitar bench mat . not rugged enough for abuse but if you take care of it , it will take care of you . makes organization of workspace much easier because screws wo n't roll around . color is good too .\n",
      "\n",
      "After Removing Stopwords:\n",
      " works great guitar bench mat . rugged abuse care , care . makes organization workspace easier screws wo roll . color good .\n",
      "\n",
      "After Removing Punctuations:\n",
      " works great guitar bench mat rugged abuse care care makes organization workspace easier screws wo roll color good\n",
      "\n",
      "After Removing Blank Spaces:\n",
      " works great guitar bench mat rugged abuse care care makes organization workspace easier screws wo roll color good\n",
      "\n",
      "Final Processed Sentence:\n",
      " works great guitar bench mat rugged abuse care care makes organization workspace easier screws wo roll color good\n",
      "\n",
      "--- File: file4.txt ---\n",
      "\n",
      "Original Text:\n",
      " Great price and good quality.  It didn't quite match the radius of my sound hole but it was close enough.\n",
      "\n",
      "After Lowercasing:\n",
      " great price and good quality.  it didn't quite match the radius of my sound hole but it was close enough.\n",
      "\n",
      "After Tokenization:\n",
      " great price and good quality .   it did n't quite match the radius of my sound hole but it was close enough .\n",
      "\n",
      "After Removing Stopwords:\n",
      " great price good quality .   match radius sound hole close .\n",
      "\n",
      "After Removing Punctuations:\n",
      " great price good quality   match radius sound hole close\n",
      "\n",
      "After Removing Blank Spaces:\n",
      " great price good quality match radius sound hole close\n",
      "\n",
      "Final Processed Sentence:\n",
      " great price good quality match radius sound hole close\n",
      "\n",
      "--- File: file5.txt ---\n",
      "\n",
      "Original Text:\n",
      " I bought this bass to split time as my primary bass with my Dean Edge. This might be winning me over. The bass boost is outstanding. The active pickups really allow you to adjust to the sound you want. I recommend this for anyone. If you're a beginner  like I was not too long ago, it's an excellent bass to start with. If you're on tour and/or music is making you money, this bass will be beatiful on stage. The color is a bit darker than in the picture. But, all around, this is a great buy.\n",
      "\n",
      "After Lowercasing:\n",
      " i bought this bass to split time as my primary bass with my dean edge. this might be winning me over. the bass boost is outstanding. the active pickups really allow you to adjust to the sound you want. i recommend this for anyone. if you're a beginner  like i was not too long ago, it's an excellent bass to start with. if you're on tour and/or music is making you money, this bass will be beatiful on stage. the color is a bit darker than in the picture. but, all around, this is a great buy.\n",
      "\n",
      "After Tokenization:\n",
      " i bought this bass to split time as my primary bass with my dean edge . this might be winning me over . the bass boost is outstanding . the active pickups really allow you to adjust to the sound you want . i recommend this for anyone . if you 're a beginner   like i was not too long ago , it 's an excellent bass to start with . if you 're on tour and/or music is making you money , this bass will be beatiful on stage . the color is a bit darker than in the picture . but , all around , this is a great buy .\n",
      "\n",
      "After Removing Stopwords:\n",
      " bought bass split time primary bass dean edge . winning . bass boost outstanding . active pickups allow adjust sound want . recommend . beginner   like long ago , excellent bass start . tour and/or music making money , bass beatiful stage . color bit darker picture . , , great buy .\n",
      "\n",
      "After Removing Punctuations:\n",
      " bought bass split time primary bass dean edge winning bass boost outstanding active pickups allow adjust sound want recommend beginner   like long ago excellent bass start tour and/or music making money bass beatiful stage color bit darker picture great buy\n",
      "\n",
      "After Removing Blank Spaces:\n",
      " bought bass split time primary bass dean edge winning bass boost outstanding active pickups allow adjust sound want recommend beginner like long ago excellent bass start tour and/or music making money bass beatiful stage color bit darker picture great buy\n",
      "\n",
      "Final Processed Sentence:\n",
      " bought bass split time primary bass dean edge winning bass boost outstanding active pickups allow adjust sound want recommend beginner like long ago excellent bass start tour and/or music making money bass beatiful stage color bit darker picture great buy\n",
      "\n",
      "--- File: file7.txt ---\n",
      "\n",
      "Original Text:\n",
      " Absolute BEST guitar hangers on the market... You will not beat this price! Buy them while you still can for this cheap\n",
      "\n",
      "After Lowercasing:\n",
      " absolute best guitar hangers on the market... you will not beat this price! buy them while you still can for this cheap\n",
      "\n",
      "After Tokenization:\n",
      " absolute best guitar hangers on the market ... you will not beat this price ! buy them while you still can for this cheap\n",
      "\n",
      "After Removing Stopwords:\n",
      " absolute best guitar hangers market ... beat price ! buy cheap\n",
      "\n",
      "After Removing Punctuations:\n",
      " absolute best guitar hangers market beat price buy cheap\n",
      "\n",
      "After Removing Blank Spaces:\n",
      " absolute best guitar hangers market beat price buy cheap\n",
      "\n",
      "Final Processed Sentence:\n",
      " absolute best guitar hangers market beat price buy cheap\n"
     ]
    }
   ],
   "source": [
    "# function to print text after each pre-processing step\n",
    "\n",
    "def print_preprocessed_files(input_folder, target_files):\n",
    "    for file_name in target_files:\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            print(f\"\\n--- File: {file_name} ---\")\n",
    "            print(\"\\nOriginal Text:\\n\", text)\n",
    "\n",
    "            text_low = text.lower()\n",
    "            print(\"\\nAfter Lowercasing:\\n\", text_low)\n",
    "\n",
    "            doc = nlp(text_low)\n",
    "            tokens = [token.text for token in doc]\n",
    "            print(\"\\nAfter Tokenization:\\n\", ' '.join(tokens))\n",
    "\n",
    "            tok_no_stop = [token for token in tokens if not nlp.vocab[token].is_stop]\n",
    "            print(\"\\nAfter Removing Stopwords:\\n\", ' '.join(tok_no_stop))\n",
    "\n",
    "            tokens_no_punct = [token for token in tok_no_stop if not nlp.vocab[token].is_punct]\n",
    "            print(\"\\nAfter Removing Punctuations:\\n\", ' '.join(tokens_no_punct))\n",
    "\n",
    "            tok_no_blank = [token for token in tokens_no_punct if token.strip()]\n",
    "            print(\"\\nAfter Removing Blank Spaces:\\n\", ' '.join(tok_no_blank))\n",
    "\n",
    "            final_sent = ' '.join(tok_no_blank)\n",
    "            print(\"\\nFinal Processed Sentence:\\n\", final_sent)\n",
    "        else:\n",
    "            print(f\"File {file_name} does not exist in the specified folder.\")\n",
    "\n",
    "input_folder = \"text\"  # Update this to the correct path\n",
    "target_files = ['file1.txt', 'file2.txt', 'file4.txt', 'file5.txt', 'file7.txt']\n",
    "\n",
    "print_preprocessed_files(input_folder, target_files)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
